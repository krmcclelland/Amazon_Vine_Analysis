{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd945495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "The system cannot find the path specified.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: $SPARK_VERSION-bin-hadoop2.7.tgz: Cannot open: No such file or directory\n",
      "tar: Error is not recoverable: exiting now\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unable to find py4j in /content/spark-3.0.1-bin-hadoop2.7\\python, your SPARK_HOME may not be configured correctly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             \u001b[0mpy4j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lib\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"py4j-*.zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23764/2253920277.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Start a SparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[0mpy4j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lib\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"py4j-*.zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             raise Exception(\n\u001b[0m\u001b[0;32m    162\u001b[0m                 \"Unable to find py4j in {}, your SPARK_HOME may not be configured correctly\".format(\n\u001b[0;32m    163\u001b[0m                     \u001b[0mspark_python\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Unable to find py4j in /content/spark-3.0.1-bin-hadoop2.7\\python, your SPARK_HOME may not be configured correctly"
     ]
    }
   ],
   "source": [
    "# Module 16.4.1 PySpark in Google Colab Notebooks\n",
    "import os\n",
    "# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.0.1'\n",
    "spark_version = 'spark-3.0.1'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49014f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 16.4.2 Spark DataFrames and Datasets\n",
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameBasics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db05138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = spark.createDataFrame([\n",
    "                                   (0,\"Here is our DataFrame\"),\n",
    "                                   (1, \"We are making one from scratch\"),\n",
    "                                   (2,\"This will look very similar to a Panda DataFrame\")\n",
    "], [\"id\",\"words\"])\n",
    "\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d84f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url = \"https://s3.amazonaws.com/dataviz-curriculum/day_1/food.csv\"\n",
    "spark.sparkContext.addFile(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc09356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50a4820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print our schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81587420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65eb722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe our data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d945b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import struct fields that we can use\n",
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5436bc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we need to create the list of struct fields\n",
    "schema = [StructField(\"food\", StringType(), True), StructField(\"price\", IntegerType(), True),]\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aae8837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in our fields\n",
    "final = StructType(fields=schema)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da953f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read our data with our new schema\n",
    "dataframe = spark.read.csv(SparkFiles.get(\"food.csv\"), schema=final, sep=\",\", header=True)\n",
    "dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b7c480",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee26ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataframe['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6c7e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.select('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55328323",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataframe.select('price'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccf2e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.select('price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c65f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column\n",
    "dataframe.withColumn('newprice', dataframe['price']).show()\n",
    "# Update column name\n",
    "dataframe.withColumnRenamed('price','newerprice').show()\n",
    "# Double the price\n",
    "dataframe.withColumn('doubleprice',dataframe['price']*2).show()\n",
    "# Add a dollar to the price\n",
    "dataframe.withColumn('add_one_dollar',dataframe['price']+1).show()\n",
    "# Half the price\n",
    "dataframe.withColumn('half_price',dataframe['price']/2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e267c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 16.4.3 Spark Functions\n",
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameFunctions\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f26f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url =\"https://s3.amazonaws.com/dataviz-curriculum/day_1/wine.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"wine.csv\"), sep=\",\", header=True)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2287d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order a DataFrame by ascending values\n",
    "df.orderBy(df[\"points\"].desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eceeff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.orderBy(df[\"points\"].desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e3b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skill Drill 16.4.3\n",
    "df.orderBy(df[\"points\"].asc()).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3e36b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Functions\n",
    "from pyspark.sql.functions import avg \n",
    "df.select(avg(\"points\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ac8dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter SQL method\n",
    "df.filter(\"price<20\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97cedac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Python method\n",
    "df.filter(\"price<20\").show(5)\n",
    "# Filter by price on certain columns\n",
    "df.filter(\"price<20\").select(['points','country', 'winery','price']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dab3088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter on exact state\n",
    "df.filter(df[\"country\"] == \"US\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb400412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skill Drill 16.4.3 \n",
    "# Filter Python method\n",
    "df.filter(\"price>15\").show(5)\n",
    "# Filter by price on certain columns\n",
    "df.filter(\"price>15\").select(['points','country', 'winery','price']).show(5)\n",
    "\n",
    "# Filter on exact state\n",
    "df.filter(df[\"province\"] == \"California\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b449884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skill Drill 16.4.3 \n",
    "# Filter Python method\n",
    "df.filter(\"price>15\").show(5)\n",
    "# Filter by price on certain columns\n",
    "df.filter(\"price>15\").select(['points','country', 'winery','price']).show(5)\n",
    "\n",
    "# Filter on exact state\n",
    "df.filter(df[\"province\"] == \"California\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
