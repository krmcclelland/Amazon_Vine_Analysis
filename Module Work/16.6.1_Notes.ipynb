{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b6eb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 16.6.1\n",
    "import os\n",
    "# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.0.1'\n",
    "spark_version = 'spark-3.2.1'\n",
    "os.environ['spark-3.2.1']=spark_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c7e8bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "The system cannot find the path specified.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
      "tar: Error is not recoverable: exiting now\n",
      "The syntax of the command is incorrect.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a2b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f6f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae6691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Tokens\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676be6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef756f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample Dataframe\n",
    "dataframe = spark.createDataFrame([\n",
    "                                   (0, \"Spark is great\"),\n",
    "                                   (1,\"We are learning Spark\"),\n",
    "                                   (2, \"Spark is better than hadoop no doubt\")\n",
    "],[\"id\",\"sentence\"])\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd18fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80eaebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform and show DataFrame\n",
    "tokenized_df = tokenizer.transform(dataframe)\n",
    "tokenized_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d6f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to return the length of a list\n",
    "def word_list_length(word_list):\n",
    "    return len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a513e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a6e880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user defined function\n",
    "count_tokens = udf(word_list_length, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecb10fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\",outputCol=\"words\")\n",
    "\n",
    "# Transform and show DataFrame\n",
    "tokenized_df = tokenizer.transform(dataframe)\n",
    "\n",
    "# Select the needed columns and don't truncate results\n",
    "tokenized_df.withColumn(\"tokens\",count_tokens(col(\"words\"))).show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
